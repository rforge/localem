```{r vignetteHeader, eval = FALSE, purl = FALSE, include = FALSE}

'
%\VignetteEngine{knitr::docco_linear} 
%\VignetteIndexEntry{Local-EM Example with Kentucky}
'

```

```{r knitrSetup, echo = FALSE, purl = FALSE, results = 'asis'}

knitr::opts_chunk$set(
    dev = 'png',
    fig.width = 4, fig.height = 2.5,
    dpi = 100, out.width = '45%', fig.ncol = 2)

if(requireNamespace('Pmisc', quietly = TRUE) & basename(getwd()) == 'www') {

    knitr::knit_hooks$set(plot = Pmisc::hook_plot_mdsubfig)
    Pmisc::markdownHeader(
        title = 'High Resolution Local-EM Example with Kentucky', 
        author = 'Patrick Brown, Paul Nguyen')
} else {

    cat('\n# Local-EM Example with Kentucky\n')
    knitr::knit_hooks$set(plot = knitr::hook_plot_html)
}

```

	
## Introduction 

The `localEM` package contains functions to implement the kernel smoothing local-EM algorithm$^1$ of disease data aggregated to geographical regions. This algorithm provides an nonparametric alternative to the standard geospatial models, such as the Besag-York-Mollie (BYM) model$^2$, for estimating spatial risk of areal disease data. With disease cases typically aggregated to highly coarse geographical regions (e.g., census counties, or census subdivisions), the local-EM method creates a tessellation of distinct regions by overlaying the map of these coarse regions with another map containing fine geographical regions (e.g., census tracts, census blocks, or census dissemination areas) of population data. This allows for the spatial risk to be estimated at a better resolution with the fine regions. 

The methodology of this package is demonstrated on simulated lung cancer cases for the state of Kentucky, USA. The spatial polygons for the census counties and tracts of Kentucky are included with this package. 

```{r setup, message = FALSE}

# specify number of grid cells and number of cores for computations in parallel 
ncores = 2
cellsFine = 80
cellsCoarse = 8
nsim = 4
nxv = 4
Sbw = seq(10, 35, by = 5) * 1000
threshold = c(1, 1.25, 1.5, 2)
Nboot = 100 

# on unix systems results will be saved in /tmp/localEMusername
path = file.path(dirname(tempdir()),  paste('localEM', Sys.info()['user'], sep = ''))

# specify number of grid cells for simulation
cellsSimulate = 200
if(!requireNamespace('RandomFields', quietly = TRUE)) {
    cellsSimulate = 100
}

dir.create(path, showWarnings = FALSE, recursive = TRUE)
dir.create('cache', showWarnings = FALSE, recursive = TRUE)

```

<!--
```{r setupOld, message = FALSE}

# specify number of grid cells and number of cores for computations in parallel 
if(any(commandArgs() == 'highRes' | basename(getwd()) == 'www')) {

    # this uses about 20GB of memory
    cellsFine = 400
    cellsSimulate = 600
    cellsCoarse = 20
    nsim = 20
    nxv = 12
    path = paste('/store/', Sys.info()['user'], '/localem/highRes', sep = '')
    Sbw = unique(round(exp(seq(log(5), log(50), len=15)))) * 1000
    threshold = c(1, 1.25, 1.5, 2)
    Nboot = 400

    knitr::opts_chunk$set(dpi = 200)

    # number of cores set to total available, min 2, max 32
    if(.Platform$OS.type == 'unix') {
        ncores = pmin(32, parallel::detectCores())
    }
} else {

    # a less computationally intensive setup
    ncores = 2
    cellsFine = 80
    cellsSimulate = 200
    cellsCoarse = 8
    nsim = 4
    nxv = 4
    # on unix systems results will be saved in /tmp/localEMusername
    path = file.path(dirname(tempdir()),  paste('localEM', Sys.info()['user'], sep = ''))
    Sbw = seq(10, 35, by = 5) * 1000
    threshold = c(1, 1.25, 1.5, 2)
    Nboot = 100 
}

if(!requireNamespace('RandomFields', quietly = TRUE)) {
    cellsSimulate = 100
}

dir.create(path, showWarnings = FALSE, recursive = TRUE)
dir.create('cache', showWarnings = FALSE, recursive = TRUE)

```
-->

```{r data}

library('mapmisc')
library('rgdal')

data('kentuckyCounty', package = 'localEM') 
data('kentuckyTract', package = 'localEM') 
data('kMap', package = 'localEM')

```


## Simulate Cases

Using the `simLgcp()` function from the `geostatsp` package, case locations are simulated with the log Gaussian Cox process (LGCP) and following parameters: 

* mean: 0
* variance: 0.16
* shape: 2
* range: 120 km
* offsets: log of expected cases/m$^2$ of the census tracts

```{r simcases, message = FALSE, warning = FALSE, cache = (basename(getwd()) == 'www'), cache.path = 'cache'}

kentuckyOffset = geostatsp::spdfToBrick(
    kentuckyTract,
    geostatsp::squareRaster(kentuckyTract, cellsSimulate),
    pattern = '^expected$',
    logSumExpected = TRUE)

set.seed(0)
kCases = geostatsp::simLgcp(
    param = c(mean = 0, variance = 0.4^2, range = 120 * 1000, shape = 2),
    covariates = list(logExpected = kentuckyOffset), 
    offset = 'logExpected', n = nsim)

```


The simulated cases are then aggregated to the appropriate counties. Plots of the relative intensity, event locations and aggregated data are provided for the first simulated dataset. 

```{r aggregateCases}

kCases$agg = lapply(
    kCases[grep('^events[[:digit:]]+?', names(kCases))],
    function(qq) over(qq, kentuckyCounty)[,'id'])
countyCounts = as.data.frame(lapply(
    kCases$agg,  
    function(xx) as.vector(table(xx, exclude = NULL)[as.character(kentuckyCounty$id)])))
countyCounts[is.na(countyCounts)] = 0
names(countyCounts) = gsub('^events', 'count', names(countyCounts))
rownames(countyCounts) = as.character(kentuckyCounty$id)
kentuckyCounty = merge(kentuckyCounty, countyCounts, by.x = 'id', by.y = 'row.names')

```

```{r plotOffset, echo = FALSE, fig.cap = 'Events for Simulation 1', fig.subcap = c('Offset', 'Relative Intensity', 'Events', 'Counts'), fig.height = 4, fig.width = 6, out.width = '80%', fig.ncol = NULL}

# offset map
oCol = colourScale(
    c(0, exp(maxValue(kentuckyOffset))), 
    breaks = 10, style = 'equal', dec = 7, 
    transform = 'sqrt')

map.new(kentuckyTract, 
    mar = c(0, 0, 1, 0), 
	  main = 'Offsets')
plot(kentuckyOffset, 
    col = oCol$col, 
    breaks = pmax(-100,log(oCol$breaks)), 
    legend = FALSE, 
    add = TRUE)
plot(kMap, add = TRUE)
legendBreaks('topleft', 
    breaks = oCol$breaks * 10^6, col = oCol$col, 
    title = expression(plain('cases/km')^2), 
    bg = 'white')


# simulated risk map
iCol = colourScale(
    kCases$raster$relativeIntensity1, 
    breaks = 8, style = 'equal', dec = -log10(0.5))

map.new(kentuckyTract, 
    mar = c(0, 0, 1, 0), 
	  main = 'Simulated Risk')
plot(kCases$raster$relativeIntensity1, 
    col = iCol$col, breaks = iCol$breaks, 
    legend = FALSE, 
    add = TRUE)
plot(kMap, add = TRUE)
legendBreaks('topleft', 
    col = iCol$col, breaks = iCol$breaks, 
    title = 'RR', 
    bg = 'white')


# simulated events map
map.new(kentuckyTract, 
    mar = c(0, 0, 1, 0), 
	  main = 'Simulated Events')
plot(kMap, add = TRUE)
points(kCases$events1, col = '#FF000030', pch = 20, cex = 0.5)
scaleBar(kentuckyCounty, 
    pos = 'topleft', 
    bg = 'white')


# simulated counts
cCol = colourScale(
    kentuckyCounty$count1, 
    breaks = 10, style = 'quantile', dec = -1)

map.new(kentuckyTract, 
    mar = c(0, 0, 1, 0), 
    main = 'Aggregated Counts')
plot(kentuckyCounty, col = cCol$plot, add = TRUE)
plot(kMap, add = TRUE)
legendBreaks('topleft', cCol)
scaleBar(kentuckyCounty, 
    pos = 'topleft', 
    inset = c(0.2, 0), 
    bg = 'white')

```


## Cross-validation

The local-EM algorithm requires a smoothing parameter called the bandwidth to estimate the spatial risk. Small values of the bandwidth yield estimates similar to standardized incidence ratios of each areal regions, while large values yield estimates to the overall mean incidence ratio of the entire study area (i.e., [total counts]/[total offsets]). The preferred or optimal bandwidth for the disease data is one that minimizes the trade-off between the bias and variance of the estimator. 

To automatic the selection of the optimal bandwidth, the `lemXv()` function of this package implements a likelihood cross-validation (CV) approach with the set of specified bandwidths. CV scores are computed with $k$-fold sampling without replacement of the dataset. The optimal bandwidth is the one that yields the smallest CV score. 

The CV scores with 4-fold sampling are provided for the first simulated dataset. An optimal bandwidth of 15 km is preferrable for the local-EM risk estimation of this dataset. 

```{r cvOne, warning = FALSE}

library('localEM')

fileHere = file.path(path, 'xvKentucky.RData')

if(!file.exists(fileHere)) {

    xvKentucky = lemXv(
        cases = kentuckyCounty[,c('id','count1')], 
        population = kentuckyTract, 
        cellsCoarse = cellsCoarse,  
        cellsFine = cellsFine, 
        bw = Sbw, 
        xv = nxv, 
        ncores = ncores, 
        path = path, 
        verbose = TRUE)

    save(xvKentucky, file = fileHere)
} else {

    load(fileHere)
}

```

```{r cvPlotOne, echo = FALSE, fig.cap = 'Cross-validation Scores for Simulation 1', fig.height = 4, fig.width = 6, out.width = '80%', fig.ncol = NULL}

plot(xvKentucky$xv[,1]/1000, xvKentucky$xv[,2], 
    main = 'CV Scores for Simulation 1', 
    xlab = 'km', ylab = '-log p', 
    type = 'o')

```


Using the R objects created from the `lemXv()` function for the first simulated dataset, this CV method can be efficiently implemented on the remaining simulated datasets. The CV scores are provided for all simulated datasets, yielding optimal bandwidths ranging from 15 to 30 km. 

```{r cvAll, warning = FALSE, cache = (basename(getwd()) == 'www'), cache.path = 'cache'}

xvAllKentucky = lemXv(
    cases = kentuckyCounty@data[,grep('^count[[:digit:]]', names(kentuckyCounty))], 
    lemObjects = xvKentucky$smoothingMatrix, 
    ncores = ncores, 
    path = 'cache', 
    verbose = TRUE)

```

```{r cvPlotAll, echo = FALSE, fig.cap = 'Cross-validation Scores for All Simulations', fig.height = 4, fig.width = 6, out.width = '80%', fig.ncol = NULL}

maxY = min(max(xvAllKentucky$xv[,-1]), 30)

matplot(xvAllKentucky$xv[,'bw']/1000, xvAllKentucky$xv[,-1], 
    main = 'CV Scores for All Simulations', 
    xlab = 'km', ylab = '-log p', 
    type = 'l', lty = 1, col = 1:4, 
    ylim = c(0, maxY))
legend('topright', paste('Simulation ', 1:4, sep = ''), 
    lty = 1, col = 1:4, 
    cex = 0.6)

```


## Risk Estimation

The R objects created from the `lemXv()` function also contain the local-EM risk estimation done with the optimal bandwidth found in the CV approach. High-resolution plots (i.e., raster resolution of 8.5x8.5 km) of the local-EM estimation with their optimal bandwidths are provided for all simulated datasets. 

```{r plotRiskAll, echo = FALSE, fig.cap = 'Risk Estimation with Optimal Bandwidths', fig.subcap = paste('Simulation ', 1:4, sep = ''), fig.height = 4, fig.width = 6, out.width = '80%', fig.ncol = NULL}

toPlot = xvAllKentucky$riskEst

rCol = colourScale(
    toPlot, 
    breaks = 8, style = 'equal', dec = 2)

bw = as.numeric(gsub('^bw', '', xvAllKentucky$bw))

for(inR in 1:nlayers(toPlot)) {

    map.new(kentuckyCounty, 
        mar = c(0, 0, 1, 0), 
        main = paste('Simulation ', inR, ': Estimated Risk, bw=', bw[inR] / 1000, ' km', sep = ''))
    plot(toPlot[[inR]], 
        col = rCol$col, breaks = rCol$breaks, 
        legend = FALSE, 
        add = TRUE)
    plot(kMap, add = TRUE)
    legendBreaks('topleft', rCol, bg = 'white')
}

```


The `riskEst()` function allows local-EM risk estimation to be done with specified bandwidths. Plots of the local-EM estimation with all bandwidths used in this example are provided for the first simulated dataset. The plots show high cancer risk in the areas located east of Bowling Green and south of Frankfurt decreasing as the bandwidth increases. 

```{r riskOne, warning = FALSE}

riskKentucky = riskEst(
    cases = kentuckyCounty[,c('id','count1')], 
    lemObjects = xvKentucky$smoothingMatrix, 
    bw = Sbw, 
    ncores = ncores, 
    path = path, 
    verbose = TRUE)

```

```{r plotRiskOne, fig.cap = 'Risk Estimation for Simulation 1', fig.subcap = paste('Risk, bw=', Sbw / 1000, ' km', sep = ''), fig.height = 4, fig.width = 6, out.width = '80%', fig.ncol = NULL}

# estimated risk maps
toPlot = riskKentucky$riskEst

rCol = colourScale(
    toPlot, 
    breaks = 8, style = 'equal', dec = 2)

for(inR in 1:nlayers(toPlot)) {

    map.new(kentuckyCounty, 
        mar = c(0, 0, 1, 0), 
    	  main = paste('Simulation 1: Estimated Risk, bw=', Sbw[inR] / 1000, ' km', sep = ''))
    plot(toPlot[[inR]], 
        col = rCol$col, breaks = rCol$breaks, 
        legend = FALSE, 
        add = TRUE)
    plot(kMap, add = TRUE)
    legendBreaks('topleft', rCol, bg = 'white')
}

```
  

## Uncertainty Estimation

To measure the uncertainty of the local-EM algorithm, the `excProb()` function computes the exceedance probabilities with the same bandwidth parameter used in the risk estimation. Bootstrapping from a Poisson process is used to simulate the events for calculating these exceedance probabilities. 

Specifically, under the assumption that disease events are a realisation from the background population and constant risk threshold, cases are bootstrapped and randomly aggregated to the areal regions. Using the same bandwidth as the observed data, the local-EM risk is then estimated for each of the bootstrapped data. Afterwards, exceedance probabilities are computed as the proportion of the observed risk estimate at least as large as the ones of the bootstrap data. Large exceedance probabilities are consistent with the risk being greater than the specified threshold. 

Exceedance probabilities with risk thresholds of 1, 1.25, 1.5 and 2 are provided for the first simulated dataset. The plots show the area east of Bowling Green and south of Frankfurt having high lung cancer risk with the estimated risk being higher than most bootstrap samples with the threshold of 1.5; however, the estimated risk does not appear to exceed a threshold of 2. 

```{r excProbOne, warning = FALSE}

excProbKentucky = excProb(
    lemObjects = xvKentucky, 
    threshold = threshold, 
    Nboot = Nboot, 
    ncores = ncores, 
    path = path, 
    verbose = TRUE)

```

```{r plotExcProbOne, echo = FALSE, fig.cap = 'Exceedance Probabilities for Simulation 1', fig.subcap = paste('Threshold ', threshold, sep = ''), fig.height = 4, fig.width = 6, out.width = '80%', fig.ncol = NULL}

toPlot = excProbKentucky$excProb

pCol = colourScale(
    toPlot, 
    breaks = c(0, 0.2, 0.8, 0.95, 1), style = 'fixed', 
    col = c('green', 'yellow', 'orange', 'red'))								

for(inT in 1:nlayers(toPlot)) {

    map.new(kentuckyCounty, 
        mar = c(0, 0, 1, 0), 
    	  main = paste('Simulation 1: Exceedance Probabilities, t=', threshold[inT], sep = ''))
    plot(toPlot[[inT]], 
        col = pCol$col, breaks = pCol$breaks, 
        legend = FALSE, 
        add = TRUE)
    plot(kMap, add = TRUE)
}
legendBreaks('topleft', pCol, bg = 'white')

```

## References

1. Nguyen P, Brown PE, Stafford J. Mapping cancer risk in southwestern Ontario with changing census boundaries. Biometrics. 2012; 68(4): 1229-37. 

2. Besag J, York J, Mollie A. Bayesian image restoration, with two applications in spatial statistics. Ann Inst Statist Math. 1991; 43(1): 1-59. 
