<!--
%\VignetteEngine{knitr::docco_linear}
%\VignetteIndexEntry{Local-EM Example with Cancer Cases in Kentucky}
-->
	
# Local-EM Example with Cancer Cases in Kentucky
	
## Introduction 

The `localEM` package contains functions to implement the kernel smoothing local-EM algorithm$^1$ of disease data aggregated to geographical regions. This algorithm provides an nonparametric alternative to the standard geospatial models, such as the Besag-York-Mollie (BYM) model$^2$, for estimating spatial risk of areal disease data. With disease cases typically aggregated to highly coarse geographical regions (e.g., census counties, or census subdivisions), the local-EM method creates a tessellation of distinct regions by overlaying the map of these coarse regions with another map containing fine geographical regions (e.g., census tracts, census blocks, or census dissemination areas) of population data. This allows for the spatial risk to be estimated at a better resolution with the fine regions. 

The methodology of this package is demonstrated on simulated lung cancer cases for the state of Kentucky, USA. The spatial polygons for the census counties and tracts of Kentucky are included with this package. 

```{r setup, message = FALSE}

library('localEM')
library('mapmisc')
library('parallel')

data('kentuckyCounty') 
data('kentuckyTract')
data('kMap')

# number of cores for computations in parallel 
# (only applicable to UNIX operating systems)
ncores = ifelse(.Platform$OS.type == 'unix', min(detectCores(), 2), 1)

```

## Rasters

The case data are observed at the county level (coarse polygon regions) while the population data are measured at the census tract level (fine polygon regions). The expected counts are computed with the age-sex population of the census tracts and rates from http://www.cancer-rates.info/, and then, scaled appropriately for this example. 

The tessellation of distinct regions, also known as partitions, is generated with the `lemRaster()` function. The census counties and tracts are first rasterized with different resolutions, and then, these coarse and fine rasters are overlayed to create the partitions of interest. This function also generates the focal weight matrix of the Gaussian smoothing kernel for specified bandwidths to be used for the smoothing matrix computations. 

```{r rasters}

# total expected events with fine polygons
kentuckyTract$expected = (3000 / sum(kentuckyTract$expected, na.rm = TRUE)) * 
  kentuckyTract$expected
sum(kentuckyTract$expected, na.rm = TRUE)

lemRaster = rasterPartition(
	polyCoarse = kentuckyCounty, 
	polyFine = kentuckyTract, 
  cellsCoarse = 10, 
	cellsFine = 120, 
  bw = c(8, 10, 12.5, 15, 17.5, 20, 25, 30) * 1000, 
  ncores = ncores,
	idFile = 'id.grd', 
	offsetFile = 'off.grd', 
	verbose = TRUE)

# total expected events with fine raster
sum(values(lemRaster$offset$offset), na.rm = TRUE) * prod(res(lemRaster$offset))

```

```{r plotOffset, echo = FALSE}

# offset map
oCol = colourScale(lemRaster$offset$offset, 
	breaks = 10, style = 'equal', dec = 8, 
	transform = 'sqrt')

map.new(kentuckyTract, 
	mar = c(0, 0, 1, 0), 
	main = 'Expected Counts')
plot(lemRaster$offset$offset, 
	col = oCol$col, breaks = oCol$breaks, 
	legend = FALSE, 
	add = TRUE)
plot(kMap, add = TRUE)

legendBreaks('topleft', 
	breaks = oCol$breaks * 10^6, col = oCol$col, 
	title = expression(plain('cases/km')^2), 
	bg = 'white')
	
scaleBar(kentuckyCounty,
	pos = 'topleft', 
	inset = c(0.3, 0.1), 
	bg = 'white')

```


## Simulate Cases

Using the `simLgcp()` function from the `geostatsp` package, case locations are simulated with the log Gaussian Cox process and  following parameters: 

* mean: 0
* variance: 0.16
* shape: 2
* range: 120 km
* offsets: log of expected cases/m$^2$ of the census tracts

The simulated cases are then aggregated to the appropriate counties. 

```{r simcases, message = FALSE}

# expected counts per squared meter
kLogOffset = log(lemRaster$offset$offset)
names(kLogOffset) = 'logOffset'

# log Gaussian Cox process simulation
set.seed(0)
kCases = geostatsp::simLgcp(
	param = c(mean = 0, variance = 0.4^2, range = 120 * 1000, shape = 2),
	covariates = kLogOffset, 
	offset = 'logOffset')
	
# simulated events aggregated to coarse polygons
countyCounts = table(over(kCases$events, kentuckyCounty)$id)
kentuckyCounty$count = countyCounts[kentuckyCounty$id]
kentuckyCounty$count[is.na(kentuckyCounty$count)] = 0

# total simulated events
sum(kentuckyCounty$count)

```

```{r plotCases, echo = FALSE}

# simulated risk map
iCol = colourScale(kCases$raster$relativeIntensity, 
	breaks = 8, style = 'equal', dec = 1)

map.new(kentuckyTract, 
	mar = c(0, 0, 1, 0), 
	main = 'Simulated Relative Risk')

plot(kCases$raster$relativeIntensity, 
	col = iCol$col, breaks = iCol$breaks, 
	legend = FALSE, 
	add = TRUE)
plot(kMap, add = TRUE)
		
legendBreaks('topleft', 
	col = iCol$col, breaks = iCol$breaks, 
	title = 'relative risk', 
	bg = 'white')

scaleBar(kentuckyCounty, 
	pos = 'topleft', 
	inset = c(0.3, 0.1), 
	bg = 'white')
		
# simulated events map
map.new(kentuckyTract, 
	mar = c(0, 0, 1, 0), 
	main = 'Simulated Events')
plot(kMap, add = TRUE)

points(kCases$events, col = '#FF000030', pch = 20)

scaleBar(kentuckyCounty, 
	pos = 'topleft', 
	inset = c(0.3, 0.1), 
	bg = 'white')

```


## Smoothing Matrix and Cross-validation

The smoothing matrix for the partitions and specified bandwidths are generated with the `smoothingMatrix()` function. 

```{r smoothingMatrix}

lemSmoothMat = smoothingMatrix(
	rasterObjects = lemRaster, 
	ncores = ncores, 
	verbose = TRUE)

dim(lemSmoothMat$smoothingArray)
dimnames(lemSmoothMat$smoothingArray)[[3]]

```

Afterwards, the likelihood cross-validation (CV) approach is implemented with the `lemXv()` function to select the optimal bandwidth (i.e., the one that yields the smallest CV score) from the set of specified bandwidths. CV scores are computed with k-fold sampling without replacement of the case data. 

```{r cv}

lemCv = lemXv(
	x = kentuckyCounty, 
  lemObjects = lemSmoothMat, 
	Nxv = 5, 
  ncores = ncores, 
	verbose = TRUE)

bestBw = lemCv$bw[which.min(lemCv$cv)]

```

```{r plotCv, echo = FALSE}

par(mar = c(5, 5, 1, 1))
plot(do.call(cbind, lemCv[is.finite(lemCv$bw),]), 
	xlab = 'Bandwidth (m)', ylab = 'CV', 
	main = 'CV Scores', 
	log = 'x', 
	pch = 16, col = 'red', type = 'o')
abline(v = bestBw, lty = 3)

```


# Risk Estimation

The local-EM risk estimation is done with the `riskEst()` function for the optimal bandwidth found in the CV analysis. The risk is estimated with the same resolution as the fine raster. 

```{r riskEst}

lemRisk = riskEst(
	x = kentuckyCounty, 
  lemObjects = lemSmoothMat,
  bw = bestBw, 
	ncores = ncores)

```

```{r plotRisk, echo = FALSE}

rCol = colourScale(lemRisk, 
	breaks = 10, style = 'equal', dec = 1)

map.new(kentuckyTract, 
	mar = c(0, 0, 1, 0), 
	main = 'Risk Estimation')
plot(lemRisk, 
	col = rCol$col, breaks = rCol$breaks, 
	legend = FALSE, 
	add = TRUE)
plot(kMap, add = TRUE)

legendBreaks('topleft', 
	col = rCol$col, breaks = rCol$breaks, 
	title = 'relative risk', 
	bg = 'white')

scaleBar(kentuckyCounty, 
	pos = 'topleft', 
	inset = c(0.3, 0.1), 
	bg = 'white')

```


## Exceedance Probabilities

Exceedance probabilities are computed with the `excProb()` function to measure the uncertainty of the local-EM risk estimation. Bootstrapping is used to obtain these exceedance probabilities for the local-EM algorithm. 

Specifically, under the assumption that disease events are a realisation from the background population and constant risk threshold, cases are bootstrapped and randomly aggregated to the counties. Using the same bandwidth as the observed data, the local-EM risk is then estimated for each of the bootstrapped data. Afterwards, exceedance probabilities are computed as the proportion of the observed risk estimate at least as large as the ones of the bootstrap data. Large exceedance probabilities are consistent with the risk being greater than the specified threshold. 

```{r excProb}

lemExcProb = excProb(
  lemEst = lemRisk, 
	lemObjects = lemSmoothMat, 
  threshold = c(1, 1.25, 1.5, 2), 
  Nboot = 200, 
  ncores = ncores)

```

```{r plotExcProb, echo = FALSE}

#exceedance probability maps
pCol = colourScale(lemExcProb, 
	breaks = c(0, 0.2, 0.8, 0.95, 1), style = 'fixed', 
	col = c('green', 'yellow', 'orange', 'red')
)								

for(Dthreshold in names(lemExcProb)) {
	
	threshold = gsub('threshold\\.', '', Dthreshold)
	map.new(kentuckyTract, 
		mar = c(0, 0, 1, 0), 
		main = paste('Exceedance Probabilities, t = ', threshold, sep = ''))
	plot(lemExcProb[[Dthreshold]], 
		col = pCol$col, breaks = pCol$breaks, 
		legend = FALSE, 
		add = TRUE)
	plot(kMap, add = TRUE)
	
	legendBreaks('topleft', 
		col = pCol$col, breaks = pCol$breaks, 
		title = 'prob', 
		bg = 'white')
	
	scaleBar(kentuckyCounty, 
		pos = 'topleft', 
		inset = c(0.3, 0.1), 
		bg = 'white')
}

```

## References

1. Nguyen P, Brown PE, Stafford J. Mapping cancer risk in southwestern Ontario with changing census boundaries. Biometrics. 2012; 68(4): 1229-37. 

2. Besag J, York J, Mollie A. Bayesian image restoration, with two applications in spatial statistics. Ann Inst Statist Math. 1991; 43(1): 1-59. 
