#' @title Computes the likelihood cross-validation of the raster partitions
#'
#' @description The \code{lemXv} function computes the likelihood cross-validation scores for the observed data with the input bandwidths used for the smoothing matrix. The cross-valiation test and training datasets of the observed cases are generated by k-fold sampling without replacement. 
#' 
#' @param cases Spatial polygons with case data
#' @param population Spatial polygons with population data
#' @param cellsCoarse Minimum resolution for rasterization of case data for numerical accuracy of smoothing matrix
#' @param cellsFine Minimum resolution for rasterization of population data for numerical integration of smoothing matrix
#' @param bw Vector of bandwidths
#' @param xv Number of cross-validation datasets
#' @param lemObjects List of arrays for the smoothing matrix, and raster stacks for the partition and smoothed offsets
#' @param ncores Number of cores/threads for parallel processing
#' @param iterations List of convergence tolerance, number of iterations, and use of gpuR package for running local-EM recursions
#' @param randomSeed Seed for random number generator
#' @param verbose Verbose output
#' @param path Folder for storing rasters
#' 
#' @details After using the \code{lemXv} function, a raster stack containing the IDs for the partitions is created by overlaying the spatial polygons of the case and population data. The smoothed offsets and smoothing matrix are computed for the specified bandwidths of each cross-validation set. 
#' 
#' @return The \code{lemXv} function returns a data frame of specified bandwidths and their cross-validation scores, and a raster of the risk estimation of the bandwidth with the lowest cross-validiation score. 
#'  
#' @export
lemXv = function(
    cases, 
    population, 
    cellsCoarse, 
    cellsFine, 
    bw,
    xv = 4, 
    lemObjects, 
    ncores = 1, 
    iterations = list(tol = 1e-5, maxIter = 1000, gpu = FALSE), 
    randomSeed = NULL, 
    path = getwd(), 
    verbose = FALSE
){
  
  dir.create(path, showWarnings = FALSE, recursive = TRUE)
  
  # warning messages
  if(missing(lemObjects)) { 
    if(class(cases) != 'SpatialPolygonsDataFrame') {
      stop("spatial polygons for case data must be provided if smoothing matrix not supplied")
    } 
    
    if(class(cases) == 'SpatialPolygonsDataFrame') {
      
      #names of interest for regions in the coarse shapefile
      countcol = grep('^(count|cases)', names(cases), value=TRUE, ignore.case=TRUE)
      
      if(!length(countcol)) {
        stop("column names of case data must start with 'count' or 'cases'")
      }
    }
  }
  
  # initializing parallel features (if specified)
  if(ncores > 1) {
    theCluster = parallel::makeCluster(spec=ncores, type='PSOCK', methods=TRUE)
    parallel::setDefaultCluster(theCluster)
    parallel::clusterEvalQ(theCluster, library('raster'))
  } else {
    theCluster = NULL
  }
  
  # for cross-validation scores
  if(!is.null(randomSeed)) {
    set.seed(randomSeed)
  }
  
  # parameter specifications (if not supplied)
  if(!length(iterations$tol)) iterations$tol = 1e-6
  if(!length(iterations$maxIter)) iterations$maxIter = 2000
  if(!length(iterations$gpu)) iterations$gpu = FALSE
  
  if(missing(lemObjects)) {  
    if(verbose) {
      cat(date(), "\n")
      cat("computing smoothing matrix\n")
    }
    
    ##raster partition
    xvLemRaster = rasterPartition(
        polyCoarse = cases, 
        polyFine = population, 
        cellsCoarse = cellsCoarse, 
        cellsFine = cellsFine, 
        xv = xv, 
        bw = bw, 
        ncores = theCluster, 
        path = path, 
        idFile = 'idXv.grd', 
        offsetFile = 'offsetXv.grd', 
        verbose = verbose)
    
    # smoothing matrix
    xvSmoothMat =  smoothingMatrix(
        rasterObjects = xvLemRaster, 
        ncores = theCluster, 
        path = path, 
        filename = 'smoothMatXv.grd', 
        verbose = verbose)
    
    
    # save smoothing matrix, useful in case of failure later on
    saveRDS(xvSmoothMat, file = file.path(path, 'smoothingMatrix.rds'))
    # xvSmoothMat = readRDS(file.path(path, 'smoothingMatrix.rds'))
    xvMat = xvSmoothMat$xv
    
  } else {
    if(verbose) {
      cat(date(), "\n")
      cat("using supplied smoothing matrix\n")
    }
    
    xvSmoothMat = lemObjects
    xvMat = lemObjects$xv
  }
  
  
  # checking structure of case data
  if(is.vector(cases)) {
    cases = data.frame(cases=cases)
  }
  if(is.matrix(cases)) {
    cases = as.data.frame(cases)
  }
  if(class(cases) == 'SpatialPolygonsDataFrame') {
    polyCoarse = cases
    cases = cases@data
  } else {
    polyCoarse = NULL
  }
  
  #names of interest for regions in the coarse shapefile
  countcol = grep('^(count|cases)', names(cases), value=TRUE, ignore.case=TRUE)
  # if(!length(countcol)){
  # countcol = grep("^(id|name)", names(cases), invert=TRUE, value=TRUE)[1]
  # }
  
  if(verbose) {
    cat(date(), "\n")
    cat("running local-EM for validation sets\n")
  }
  # estimate risk (by partition, not continuous) for each bw/cv combinantion
  
  forMoreArgs = list(
      x=cases[,countcol, drop=FALSE],
      lemObjects = xvSmoothMat,
      tol = iterations$tol, 
      maxIter = iterations$maxIter,
      gpu = iterations$gpu,
      verbose=verbose)
  
  if(!is.null(theCluster)) {
    estList = parallel::clusterMap(
        theCluster,
        finalLemIter,
        bw = xvSmoothMat$bw,
        MoreArgs = forMoreArgs,
        SIMPLIFY=FALSE
    )
    
  } else {
    estList = mapply(
        finalLemIter,
        bw = xvSmoothMat$bw,
        MoreArgs = forMoreArgs,
        SIMPLIFY=FALSE
    )
  }
  
  if(any(unlist(lapply(estList, class)) == 'try-error') ) {
    warning("errors in local-em estimation")
  }
  
  estListExp = try(lapply(estList, function(x) x$expected))
  
  estListRisk = lapply(estList, function(x) x$risk)
  
  estDf = as.matrix(do.call(cbind, estListExp))
  riskDf = as.matrix(do.call(cbind, estListRisk))
  colnames(estDf) = colnames(riskDf) = paste(
      rep(names(estList), unlist(lapply(estListExp, function(xx) dim(xx)[2]))),
      unlist(lapply(
              estListExp, colnames
          )), sep = '_')
  rownames(riskDf) = colnames(xvSmoothMat$regionMat)
  
  if(verbose) {
    cat("computing CV scores\n")
  }
  # compute the CV scores
  
  # expected counts in left out regions
  xvEst = estDf[,grep("xv[[:digit:]]+", colnames(estDf))]
  Sxv = gsub("^bw[[:digit:]]+xv|_count[[:digit:]]+?$|_$", "", colnames(xvEst))
  Scount = gsub("bw[[:digit:]]+xv[[:digit:]]+_", "", colnames(xvEst))
  if(all(nchar(Scount)==0)) Scount = rep_len(countcol, length(Scount))
  Sbw = gsub('^bw|xv.*', "", colnames(xvEst))
  
  suppressMessages(xvEstMask <- xvMat[,Sxv] * xvEst)
  # observed counts in left out regions
  suppressMessages(xvObs <- xvMat[,Sxv] * as.matrix(cases[,Scount]))
  
  logProbCoarse = stats::dpois(as.matrix(xvObs), as.matrix(xvEstMask), log=TRUE)
  logProb = apply(logProbCoarse, 2, sum)
  
  logProbFull = data.frame(
      bw = as.numeric(Sbw),
      cases = Scount, 
      fold = Sxv, 
      minusLogProb = -logProb
  )
  
  xvRes = stats::aggregate(
      logProbFull[,'minusLogProb'],
      as.list(logProbFull[,c('bw','cases')]),
      sum
  )
  xvRes = stats::reshape(
      xvRes, direction = 'wide',
      idvar = 'bw',
      timevar = 'cases'
  )
  colnames(xvRes) = gsub("^x[.]", "", colnames(xvRes))    
  xvRes = xvRes[,c('bw',countcol)]
  minXvScore = apply(xvRes[,countcol, drop=FALSE],2,min)
  xvRes[,countcol] = xvRes[,countcol] - 
      matrix(minXvScore, nrow=nrow(xvRes), ncol=length(minXvScore), byrow=TRUE)
  
#  stuff <<- list(xvSmoothMat$rasterFine, riskDf) 
  newDf <- riskDf[
      as.character(raster::levels(xvSmoothMat$rasterFine)[[1]]$partition),]
  
  riskRaster = xvSmoothMat$rasterFine
  levels(riskRaster)[[1]] =  as.data.frame(cbind(
          ID = raster::levels(xvSmoothMat$rasterFine)[[1]]$ID,
          newDf))
  
  
  
  result = list(
      xv = xvRes,
      xvFull = logProbFull,
      riskAll = riskRaster,
      smoothingMatrix = xvSmoothMat,
      expected = polyCoarse,
      folds = xvMat
  )
  
  if(verbose) {
    cat(date(), "\n")
    cat("computing risk estimation for bw with lowest CV score\n")
  }
  
  bwMin = result$xv[apply(result$xv[,colnames(result$xv)[-1]],2,which.min),'bw']
  Slayers = paste('bw', bwMin, '_', countcol, sep = '')

  # done with the cluster
  if(!is.null(theCluster))
    parallel::stopCluster(theCluster)
  
  result$riskEst = finalSmooth(
      x = result, 
      Slayers = Slayers, 
      filename = file.path(path, 'riskXv.grd'),
      ncores = 1)
  names(result$riskEst) = Slayers
  
  result$bw = paste('bw', bwMin, sep = '')
  
  if(verbose) {
    cat(date(), "\n")
    cat("done\n")
  }
  
  return(result)
}
