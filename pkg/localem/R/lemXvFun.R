#' @title Computes the likelihood cross-validation 
#'
#' @description The \code{lemXv} function computes the likelihood cross-validation scores for the observed data with the input bandwidths used for the smoothing matrix. 
#'  The cross-valiation test and training datasets of the observed cases are generated by k-fold sampling without replacement. 
#' 
#' @param \code{x} Spatial polygons of case data
#' @param \code{lemObjects} List of arrays for the smoothing matrix and 
#'  raster stacks for the partition and smoothed offsets
#' @param \code{Nxv} Number of cross-validation datasets
#' @param \code{ncores} Number of cores/threads for parallel processing
#' 
#' @importFrom spdep poly2nb
#' @importFrom stats dpois rmultinom
#' 
#' @details After using the \code{lemXv} function, the last entry of the data frame object is the theoretical cross-validation score when bandwidth is set to infinity. 

#' @return The \code{lemXv} function returns a data frame of specified bandwidths and cross-validation scores. 
#'  
#' @examples 
#' data(kentuckyCounty)
#' 
#' data(kentuckyTract)
#' 
#' \dontrun{
#' lemRaster = rasterPartition(polyCoarse = kentuckyCounty, polyFine = kentuckyTract, 
#'                    cellsCoarse = 40, cellsFine = 400, 
#'                    bw = c(10, 15, 20, 25) * 1000, 
#'                    ncores = 4, 
#'                    idFile = 'id.grd', offsetFile = 'offset.grd')
#'                    
#' lemSmoothMat = smoothingMatrix(rasterObjects = lemRaster, 
#'                    ncores = 4) 
#'                    
#' lemCv = lemXv(x = kentuckyCounty, 
#'                    lemObjects = lemSmoothMat, 
#'                    ncores = 4) 
#'}
#'
lemXv = function(x, 
                 lemObjects,
                 Nxv = 5, 
                 ncores = 4, 
                 tol = 1e-6, 
                 maxIter = 2000, 
                 randomSeed = NULL, 
                 verbose = FALSE
){
  
  regionMat = lemObjects$regionMat
  
  trainOffsetMat = lemObjects$offsetMat * (Nxv - 1) / Nxv
  xvOffsetMat = lemObjects$offsetMat / Nxv
  
  trainSmoothingMat = lemObjects$smoothingArray / ((Nxv - 1) / Nxv)
  
  idCoarse = lemObjects$polyCoarse$id
  
  #fine raster did not include all regions in the coarse shapefile
  if(length(idCoarse) != dim(regionMat)[[2]]) {
    
    polyNeigh = poly2nb(lemObjects$polyCoarse, row.names = idCoarse)
    
    idMatch = idCoarse[as.numeric(dimnames(regionMat)[[2]])]
    idNotMatch = idCoarse[!(idCoarse %in% idMatch)]
    
    countCoarse = x$count[match(idMatch, x[['id']])]
    
    for(inD in idNotMatch) {
      
      polyNotMatch = lemObjects$polyCoarse[idCoarse == inD,]
      idNeighNotMatch = idCoarse[values(intersect(lemObjects$rasterFine[["idCoarse"]], polyNotMatch))]
      idNeighNotMatch = idNeighNotMatch[!is.na(idNeighNotMatch)]
      
      #if no match found in fine raster, use neighbouring coarse shapefile regions 
      if(length(idNeighNotMatch) == 0) {
        idNeighNotMatch = idCoarse[polyNeigh[[which(idCoarse == inD)]]]
        idNeighNotMatch = idMatch[idMatch %in% idNeighNotMatch]
      }
      
      #re-assign counts
      if(length(idNeighNotMatch) == 1) {
        
        countCoarse[idMatch == idNeighNotMatch] = 
          countCoarse[idMatch == idNeighNotMatch] + x$count[x$id == inD]
        
      } else if(length(idNeighNotMatch) > 1) {
        
        #if conflict, assign counts to coarse shapefile region whose centroid is closest to the one of interest
        polyNeighNotMatch = lemObjects$polyCoarse[idCoarse %in% idNeighNotMatch,]
        coordsNeighNotMatch = coordinates(rgeos::gCentroid(polyNeighNotMatch, byid = TRUE))
        
        coordsNotMatch = matrix(
          rep(coordinates(rgeos::gCentroid(polyNotMatch, byid = TRUE)), each = length(polyNeighNotMatch)), 
          nrow = length(polyNeighNotMatch), 
          ncol = 2, 
          dimnames = list(1:length(polyNeighNotMatch), c("x","y"))
        )
        
        distNeighNotMatch = apply((coordsNeighNotMatch - coordsNotMatch)^2, 1, sum)
        
        countCoarse[idMatch == idNeighNotMatch[which.min(distNeighNotMatch)]] = 
          countCoarse[idMatch == idNeighNotMatch[which.min(distNeighNotMatch)]] + x$count[x$id == inD]
      }
    }
    
    idCoarse = idMatch
    
  } else {
    countCoarse = x$count[match(idCoarse, x[['id']])]
  }
  
  #partition area
  rasterFine = lemObjects$rasterFine
  
  Scw2 = prod(res(rasterFine))
  Scells = ifelse(is.na(values(rasterFine[["cellCoarse"]])) | is.na(values(rasterFine[["idCoarse"]])), 
                  NA, 
                  paste("c", values(rasterFine[["cellCoarse"]]), "p", values(rasterFine[["idCoarse"]]), sep = "")
  )
  SNcells = tapply(Scells, list(Scells), length)
  SNcells = SNcells[match(dimnames(trainSmoothingMat)[[1]], names(SNcells))]
  Sarea = SNcells * Scw2 
  
  #test set
  if(!is.null(randomSeed)) {
    set.seed(randomSeed)
  }
  xvCounts = t(mapply(rmultinom, size = countCoarse, MoreArgs = list(n = 1, prob = rep(1/Nxv, Nxv))))
  dimnames(xvCounts) = list(idCoarse, 1:Nxv)
  
  #training set
  trainCounts = matrix(0,
                       nrow = length(idCoarse), 
                       ncol = Nxv,
                       dimnames = list(idCoarse, 1:Nxv)
  )
  trainCounts[as.character(idCoarse),] = countCoarse
  trainCounts[dimnames(xvCounts)[[1]],] = trainCounts[dimnames(xvCounts)[[1]],] - xvCounts
  
  #risk estimation  
  xvLambda = array(NA, 
                   dim = c(dim(trainSmoothingMat)[1], Nxv, dim(trainSmoothingMat)[3] + 1), 
                   dimnames = list(dimnames(trainSmoothingMat)[[1]], dimnames(trainCounts)[[2]], c(dimnames(trainSmoothingMat)[[3]],"bwInf"))
  )
  
  xvRes = array(NA, 
                dim = c(dim(xvCounts)[1], Nxv, dim(xvLambda)[3]), 
                dimnames = list(dimnames(xvCounts)[[1]], dimnames(xvCounts)[[2]], dimnames(xvLambda)[[3]])
  )
  
  for(Dbw in dimnames(trainSmoothingMat)[[3]]) {
    
    if(verbose) {
      cat(date(), "\n")
      cat("obtaining CV for bandwidth: ", gsub("^bw", "", Dbw), "\n")
    }
    
    #risk estimation of training set for finite bandwidths
    xvLambda[,,Dbw] = simplify2array(
      parallel::mclapply(1:Nxv, 
                         xvLemEst, 
                         trainCounts = trainCounts, 
                         regionMat = regionMat, 
                         offsetMat = trainOffsetMat, 
                         smoothingMat = trainSmoothingMat[,,Dbw], 
                         tol = tol, 
                         maxIter = maxIter, 
                         mc.cores=ncores
      ))
    
    #likelihood cross-validation scores of test set for finite bandwidths
    xvMeans = t(regionMat) %*% xvOffsetMat %*% xvLambda[,,Dbw]
    
    for(D in 1:Nxv) {
      
      xvRes[,D,Dbw] = dpois(xvCounts[,D], xvMeans[,D], log = TRUE)
    }
  }
  
  #risk estimation of training set for infinity bandwidth
  Sexpected = diag(trainOffsetMat) * Sarea
  
  xvLambda[,,"bwInf"] = outer(Sarea, apply(trainCounts, 2, sum) / sum(Sexpected), "*")
  
  #likelihood cross-validation scores of test set for infinity bandwidth
  xvMeans = t(regionMat) %*% xvOffsetMat %*% xvLambda[,,"bwInf"]
  
  for(D in 1:Nxv) {
    
    xvRes[,D,"bwInf"] = dpois(xvCounts[,D], xvMeans[,D], log = TRUE)
  }
  
  #cross-validation scores
  xvCV = -apply(xvRes, 3, sum)
  
  result = data.frame(
    bw = as.numeric(gsub("^bw", "", dimnames(xvLambda)[[3]])), 
    cv = xvCV
  )

  return(result)
  
  if(verbose) { 
    cat(date(), "\n")
    cat("done\n")
  }
}