#' @title Computes the likelihood cross-validation 
#'
#' @description The \code{lemXv} function computes the likelihood cross-validation scores for the observed data with the input bandwidths used for the smoothing matrix. The cross-valiation test and training datasets of the observed cases are generated by k-fold sampling without replacement. 
#' 
#' @param polyCoarse, 
#' @param polyFine, 
#' @param cellsCoarse, 
#' @param cellsFine, 
#' @param bw,
#' @param Nxv Number of cross-validation datasets
#' @param lemObjects List of arrays for the smoothing matrix, and raster stacks for the partition and smoothed offsets
#' @param ncores Number of cores/threads for parallel processing
#' @param tol Tolerance for convergence
#' @param maxIter Maximum number of iterations for convergence
#' @param randomSeed Seed for random number generator
#' @param verbose Verbose output
#' @param path folder for storing rasters
#' 
#' 
#' @details After using the \code{lemXv} function, the last entry of the data frame object is the theoretical cross-validation score when bandwidth is set to infinity. 

#' @return The \code{lemXv} function returns a data frame of specified bandwidths and their cross-validation scores. 
#'  
#' @examples 
#' \dontrun{ 
#' data(kentuckyCounty)
#' data(kentuckyTract)
#' 
#' ncores = 1 + (.Platform$OS.type == 'unix')
#' 
#' lemRaster = rasterPartition(polyCoarse = kentuckyCounty, 
#'                            polyFine = kentuckyTract, 
#'                            cellsCoarse = 6, 
#'                            cellsFine = 100, 
#'                            bw = c(10, 12, 15, 17, 20, 25) * 1000, 
#'                            ncores = ncores, 
#'                            idFile = 'id.grd', 
#'                            offsetFile = 'offset.grd', 
#'                            verbose = TRUE)
#'
#'
#' lemSmoothMat = smoothingMatrix(rasterObjects = lemRaster, 
#'                                ncores = ncores, 
#'                                verbose = TRUE)
#'
#' lemCv = lemXv(x = kentuckyCounty, 
#'              lemObjects = lemSmoothMat, 
#'              Nxv = 5, 
#'              ncores = ncores, 
#'              verbose = TRUE)
#' bestBw = lemCv$bw[which.min(lemCv$cv)]
#'
#' plot(lemCv[is.finite(lemCv$bw),], 
#'     xlab = 'Bandwidth (m)', 
#'     ylab = 'CV', 
#'     log = 'x', col = 'red', type = 'o')
#' abline(v = bestBw, lty = 3)
#'}
#'
#' @export
#' Computes the likelihood cross-validation by removing spatial regions and estimating risk for those regions
#' This requires computations of new smoothing matrix for each training set
#' Sampling Ratio: 75/25% of data for training and testing sets
lemXv = function(
    polyCoarse, 
    polyFine, 
    cellsCoarse, 
    cellsFine, 
    bw,
    Nxv = 4, 
    lemObjects, 
    ncores = 1, 
    tol = 1e-6, 
    maxIter = 2000, 
    randomSeed = NULL, 
    verbose = FALSE,
    path = getwd()
){
  
  # forcross-validation scores
  if(!is.null(randomSeed)) {
    set.seed(randomSeed)
  }
  
  if(missing(lemObjects)) {  
  ##raster partition
  xvLemRaster = rasterPartition(
      polyCoarse = polyCoarse, 
      polyFine = polyFine, 
      cellsCoarse = cellsCoarse, 
      cellsFine = cellsFine, 
      xv = Nxv,
      bw = bw, 
      ncores = ncores, 
      idFile = file.path(path,'idXv.grd'), 
      offsetFile = file.path(path, 'offsetXv.grd'), 
      verbose = verbose)
  
  xvSmoothMat =  smoothingMatrix(
      rasterObjects = xvLemRaster, 
      ncores = ncores, 
      verbose = verbose)
} else{
  xvSmoothMat = lemObjects
}
  
  #names of interest for regions in the coarse shapefile
  countcol = grep('^(count|cases)$', names(polyCoarse), value=TRUE, ignore.case=TRUE)
  if(length(countcol)){
    countcol = countcol[1]
  } else {
    countcol = grep(
        "^(id|name)", names(polyCoarse), 
        invert=TRUE, value=TRUE
    )[1]
  }
  
  idColX = grep("^id", names(polyCoarse), value=TRUE)
  if(length(idColX)) {
    idColX = idColX[1]
  } else {
    idColX = names(polyCoarse)[1]
  }
  
  # estimate risk (by partition, not continuous) for each bw/cv combinantion
  estList = parallel::mcmapply(
      riskEst,
      bw = dimnames(xvSmoothMat$smoothingArray)[[3]],
      MoreArgs = list(
          x=polyCoarse@data[,countcol],
          lemObjects = xvSmoothMat,
          tol = tol, 
          maxIter =maxIter,
          ncores = NULL,
          type = 'expected'),
      SIMPLIFY=FALSE
  )
  estListExp = lapply(estList, function(x) x$expected)
  estListRisk = lapply(estList, function(x) x$risk)
  
  estDf = as.matrix(do.call(cbind, estListExp))
  riskDf = as.matrix(do.call(cbind, estListRisk))
  colnames(estDf) = colnames(riskDf) = names(estList)
  
  
  # compute the CV scores
  
  # expected counts in left out regions
  xvEst = estDf[,grep("xv[[:digit:]]+$", colnames(estDf))]
  Sxv = gsub("^bw[[:digit:]]+xv", "", colnames(xvEst))
  xvEstMask = xvMat[,Sxv] * xvEst 
  
  # observed counts in left out regins
  xvObs = xvMat * as.matrix(polyCoarse@data[,rep(countcol, ncol(xvMat))])
  xvObs = xvObs[,Sxv]
  
  
  logProbCoarse = stats::dpois(as.matrix(xvObs), as.matrix(xvEstMask), log=TRUE)
  logProb = apply(logProbCoarse, 2, sum)
  names(logProb) = colnames(xvEst)
  
  logProb = data.frame(
      bw = as.numeric(gsub("^bw|xv[[:digit:]]+$", "", names(logProb))),
      fold = gsub("^bw[[:digit:]]+xv", "", names(logProb)),
      minusLogProb = -logProb 
  )
  
  totalXv = tapply(logProb$minusLogProb, logProb$bw, sum)
  totalXv = data.frame(
      bw = as.numeric(names(totalXv)),
      fold = 'total',
      minusLogProb = totalXv
  )
  
  xvScore = rbind(logProb, totalXv[,colnames(logProb)])
  
  expCoarse = polyCoarse
  expCoarse@data = as.data.frame(as.matrix(estDf))
  
  levels(xvSmoothMat$rasterFine)[[1]] = cbind(
      levels(xvSmoothMat$rasterFine)[[1]], 
      as.matrix(riskDf[levels(xvSmoothMat$rasterFine)[[1]]$partition,])
  )
  
  result = list(
      xv = totalXv[totalXv$fold=='total', c('bw','minusLogProb')],
      xvFull = totalXv,
      smoothingMatrix = xvSmoothMat,
      expected = estDf,
      folds = xvMat
      )
  return(result)
}




#' Additional functions from the 'localEM' package that are required but were not globally loaded
## Computes the risk estimation aggregated to the partitions for one iteration
oneLemIter = function(
    Lambda, smoothingMat, regionMat, offsetMat, counts,
    regionOffset = crossprod(regionMat, offsetMat)
){
  
  denom = regionOffset %*% Lambda
  
  em = crossprod(regionOffset, counts/denom) * Lambda
  em[as.vector(!is.finite(em))] = 0
  
  result = crossprod(smoothingMat, em)
  attributes(result)$em = em
  
  return(result)
}

## Computes the expected counts for the test set based on aggregated risk estimation and smoothing matrix of training set
xvLemEstOneBw = function(
    Dbw,
    trainId,
    trainCounts,
    regionOffset, # = crossprod(regionMat, offsetMat)
    smoothingMat,
    regionXvOffset, # = crossprod(regionMat, xvOffsetMat) 
    startingValue,
    tol,
    maxIter) {
  
  xvLambda = xvLemEst(
      trainId=trainId,
      trainCounts=trainCounts,
      regionOffset =regionOffset,
      smoothingMat = Matrix(drop(smoothingMat[,,Dbw])),
      startingValue = startingValue,
      tol=tol,
      maxIter = maxIter)
  
  as.matrix(regionXvOffset %*% xvLambda)
  
}

## Computes the aggregated risk estimation of the training set
xvLemEst = function(
    trainId,
    trainCounts,
    regionOffset,
    smoothingMat,
    offsetMat,
    startingValue = matrix(
        apply(regionOffset, 2, sum),
        ncol(regionOffset), length(trainId)
    ),
    tol,
    maxIter) {
  
  if(missing(trainId))
    trainId = 1:ncol(trainCounts)
  obsCounts = as.matrix(trainCounts[,trainId, drop=FALSE])
  
  oldRes = startingValue
  
  Diter = 1
  absdiff = 1
  
  while((absdiff > tol) && (Diter < maxIter)) {
    
    newRes = oneLemIter(
        Lambda = oldRes,
        smoothingMat = smoothingMat,
        regionOffset = regionOffset,
        counts = obsCounts
    )
    
    absdiff = mean(abs(oldRes - newRes))
    
    oldRes = newRes
    Diter = Diter + 1
  }
  
  result = as.matrix(newRes)
  
  return(result)
}
