#' @title Computes the likelihood cross-validation of the raster partitions
#'
#' @description The \code{lemXv} function computes the likelihood cross-validation scores for the observed data with the input bandwidths used for the smoothing matrix. The cross-valiation test and training datasets of the observed cases are generated by k-fold sampling without replacement. 
#' 
#' @param cases Spatial polygons with case data
#' @param population Spatial polygons with population data
#' @param cellsCoarse Minimum resolution for rasterization of case data for numerical accuracy of smoothing matrix
#' @param cellsFine Minimum resolution for rasterization of population data for numerical integration of smoothing matrix
#' @param bw Vector of bandwidths
#' @param xv Number of cross-validation datasets
#' @param lemObjects List of arrays for the smoothing matrix, and raster stacks for the partition and smoothed offsets
#' @param ncores Number of cores/threads for parallel processing
#' @param iterations convergence tolerance, number of iterations, and use of gpuR package for running local-EM recursions
#' @param fact Aggregation factor for offsets prior to smoothing
#' @param randomSeed Seed for random number generator
#' @param verbose Verbose output
#' @param path Folder for storing rasters
#' 
#' @details After using the \code{lemXv} function, a raster stack containing the IDs for the partitions is created by overlaying the spatial polygons of the case and population data. The smoothed offsets and smoothing matrix are computed for the specified bandwidths of each cross-validation set. 
#' 
#' @return The \code{lemXv} function returns a data frame of specified bandwidths and their cross-validation scores, and a raster of the risk estimation of the bandwidth with the lowest cross-validiation score. 
#'  
#' @import foreach raster sp
#' @export
lemXv = function(
    cases, 
    population, 
    cellsCoarse, 
    cellsFine, 
    bw,
    xv = 4, 
    lemObjects, 
    ncores = 1, 
    iterations = list(tol = 1e-5, maxIter = 1000, gpu=FALSE), 
    fact = 1,
    randomSeed = NULL, 
    verbose = FALSE,
    path = getwd()
){
  
  dir.create(path, showWarnings=FALSE, recursive=TRUE)
  
  # forcross-validation scores
  if(!is.null(randomSeed)) {
    set.seed(randomSeed)
  }
  
  if(!length(iterations$tol)) iterations$tol = 1e-6
  if(!length(iterations$maxIter)) iterations$maxIter = 2000
  if(!length(iterations$gpu)) iterations$gpu = FALSE
  
  if(missing(lemObjects)) {  
    if(verbose) {
      cat("computing smoothing matrix\n")
    }


    if(ncores > 1) {
  		theCluster = parallel::makeCluster(spec=ncores, type='PSOCK', methods=TRUE)
		parallel::setDefaultCluster(theCluster)
	}
    
    ##raster partition
    xvLemRaster = rasterPartition(
        polyCoarse = cases, 
        polyFine = population, 
        cellsCoarse = cellsCoarse, 
        cellsFine = cellsFine, 
        xv = xv,
        bw = bw,
        fact = fact,
        ncores = 0, 
        path = path,
        idFile = file.path(path,'idXv.grd'), 
        offsetFile = file.path(path, 'offsetXv.grd'), 
        verbose = verbose)
    
    xvSmoothMat =  smoothingMatrix(
        rasterObjects = xvLemRaster, 
        ncores = 0, 
        filename = file.path(path, 'smoothingMatrix.grd'),
        verbose = verbose)
    
    if(ncores > 1)
      spatial.tools::sfQuickStop()   
    
    # save smoothing matrix, useful in case of failure later on
    saveRDS(xvSmoothMat, file = file.path(path, 'smoothingMatrix.rds'))
    
    xvMat = xvSmoothMat$xv
    
  } else {
    if(verbose) {
      cat("using supplied smoothing matrix\n")
    }
    
    xvSmoothMat = lemObjects
    xvMat = lemObjects$xv
  }
  
  if(is.vector(cases)) {
    cases = data.frame(cases=cases)
  }
  #names of interest for regions in the coarse shapefile
  countcol = grep('^(count|cases)[[:digit:]]+?$', 
      names(cases), value=TRUE, ignore.case=TRUE)
  if(!length(countcol)){
    countcol = grep(
        "^(id|name)", names(cases), 
        invert=TRUE, value=TRUE
    )[1]
  }
  if(class(cases) == 'SpatialPolygonsDataFrame') {
    polyCoarse = cases
    cases = cases@data
  } else {
    polyCoarse = NULL
  }
  
  if(verbose) {
    cat("running local-EM for validation sets\n")
  }
  # estimate risk (by partition, not continuous) for each bw/cv combinantion
  
  if(ncores > 1 & !identical(iterations$gpu, TRUE)) {
  	theCluster = parallel::makeCluster(spec=ncores, type='PSOCK', methods=TRUE)
	parallel::setDefaultCluster(theCluster)
	
	estList = parallel::clusterMap(
		theCluster,
		riskEst,
		bw = xvSmoothMat$bw,
		MoreArgs = list(
            x=cases[,countcol, drop=FALSE],
            lemObjects = xvSmoothMat,
            tol = iterations$tol, 
            maxIter = iterations$maxIter,
            gpu = iterations$gpu,
            verbose=verbose)
	)
	
    parallel::stopCluster(theCluster) #spatial.tools::sfQuickStop()
  } else {
  	estList = mapply(
		riskEst,
		bw = xvSmoothMat$bw,
		MoreArgs = list(
            x=cases[,countcol, drop=FALSE],
            lemObjects = xvSmoothMat,
            tol = iterations$tol, 
            maxIter = iterations$maxIter,
            gpu = iterations$gpu,
            verbose=verbose)
	)
  }
  #	spatial.tools::sfQuickInit(
  #    ncores, 
  #    methods = TRUE,
  #    .packages = c('Matrix','localEM', 'raster'))

  
#  estList = foreach::foreach(
#          bw = xvSmoothMat$bw,
#          .packages=c('raster','Matrix')) %dopar% {
#       try(localEM::riskEst(bw,
#            x=cases[,countcol, drop=FALSE],
#            lemObjects = xvSmoothMat,
#            tol = iterations$tol, 
#            maxIter = iterations$maxIter,
#            gpu = iterations$gpu,
#            verbose=verbose))
#      }
#  if(ncores > 1 & !identical(iterations$gpu, TRUE)) 

  names(estList) = xvSmoothMat$bw
  
  if(any(unlist(lapply(estList, class)) == 'try-error') ) {
    warning("errors in local-em estimation")
    return(list(
            estList = estList,
            smoothingMatrix = xvSmoothMat,
            expected = polyCoarse,
            folds = xvMat
        ))
  }
  
  estListExp = try(lapply(estList, function(x) x$expected))
  
  if(class(estListExp) == "try-error") {
    return(list(
            xv = NULL,
            xvFull = NULL,
            smoothingMatrix = xvSmoothMat,
            expected = polyCoarse,
            folds = xvMat
        ))
  }
  
  estListRisk = lapply(estList, function(x) x$risk)
  
  estDf = as.matrix(do.call(cbind, estListExp))
  riskDf = as.matrix(do.call(cbind, estListRisk))
  colnames(estDf) = colnames(riskDf) = paste(
    rep(names(estList), unlist(lapply(estListExp, function(xx) dim(xx)[2]))),
          unlist(lapply(
      estListExp, colnames
      )), sep='_')
  rownames(riskDf) = colnames(xvSmoothMat$regionMat)
  
  if(verbose) {
    cat("computing CV scores\n")
  }
  # compute the CV scores
  
  # expected counts in left out regions
  xvEst = estDf[,grep("xv[[:digit:]]+", colnames(estDf))]
  Sxv = gsub("^bw[[:digit:]]+xv|_count[[:digit:]]+?$|_$", "", colnames(xvEst))
  Scount = gsub("bw[[:digit:]]+xv[[:digit:]]+_", "", colnames(xvEst))
  if(all(nchar(Scount)==0)) Scount = rep_len(countcol, length(Scount))
  Sbw = gsub('^bw|xv.*', "", colnames(xvEst))
  
  suppressMessages(xvEstMask <- xvMat[,Sxv] * xvEst)
  # observed counts in left out regions
  suppressMessages(xvObs <- xvMat[,Sxv] * as.matrix(cases[,Scount]))
  
  logProbCoarse = stats::dpois(as.matrix(xvObs), as.matrix(xvEstMask), log=TRUE)
  logProb = apply(logProbCoarse, 2, sum)
  
  logProbFull = data.frame(
      bw = as.numeric(Sbw),
      cases = Scount, 
      fold = Sxv, 
      minusLogProb = -logProb
  )
  
  xvRes = stats::aggregate(
      logProbFull[,'minusLogProb'],
      as.list(logProbFull[,c('bw','cases')]),
      sum
  )
  xvRes = stats::reshape(
      xvRes, direction = 'wide',
      idvar = 'bw',
      timevar = 'cases'
  )
  colnames(xvRes) = gsub("^x[.]", "", colnames(xvRes))    
  xvRes = xvRes[,c('bw',countcol)]
  minXvScore = apply(xvRes[,countcol, drop=FALSE],2,min)
  xvRes[,countcol] = xvRes[,countcol] - 
      matrix(minXvScore, nrow=nrow(xvRes), ncol=length(minXvScore), byrow=TRUE)
  if(verbose) {
    cat("putting estimated risk in raster\n")
  }
#  stuff <<- list(xvSmoothMat$rasterFine, riskDf) 
  newDf <- riskDf[
    as.character(raster::levels(xvSmoothMat$rasterFine)[[1]]$partition),]
  
  riskRaster = xvSmoothMat$rasterFine
  levels(riskRaster)[[1]] =  as.data.frame(cbind(
    ID = raster::levels(xvSmoothMat$rasterFine)[[1]]$ID,
    newDf))
    

  
  result = list(
      xv = xvRes,
      xvFull = logProbFull,
      riskAll = riskRaster,
      smoothingMatrix = xvSmoothMat,
      expected = polyCoarse,
      folds = xvMat
  )
  
  if(verbose) {
    cat("final smoothing step\n")
  }
  
  result$estimate = try(
  	lemFinal(result), silent=TRUE
  )
  
  
  return(result)
  
  
  
  
}



