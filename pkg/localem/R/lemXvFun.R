#' @title Computes the likelihood cross-validation 
#'
#' @description The \code{lemXv} function computes the likelihood cross-validation scores for the observed data with the input bandwidths used for the smoothing matrix. The cross-valiation test and training datasets of the observed cases are generated by k-fold sampling without replacement. 
#' 
#' @param cases spatial object with case counts
#' @param population spatial object with population data
#' @param cellsCoarse number of cells for parallel computations of the smoothing matrix
#' @param cellsFine number of cells for numerical integration
#' @param bw vector of bandwidths
#' @param xv Number of cross-validation datasets
#' @param lemObjects List of arrays for the smoothing matrix, and raster stacks for the partition and smoothed offsets
#' @param ncores Number of cores/threads for parallel processing
#' @param tol Tolerance for convergence
#' @param maxIter Maximum number of iterations for convergence
#' @param fact aggregation factor for offsets prior to smoothing
#' @param randomSeed Seed for random number generator
#' @param verbose Verbose output
#' @param path folder for storing rasters
#' 
#' @details After using the \code{lemXv} function, the last entry of the data frame object is the theoretical cross-validation score when bandwidth is set to infinity. 
#' 
#' @return The \code{lemXv} function returns a data frame of specified bandwidths and their cross-validation scores. 
#'  
#' @export
lemXv = function(
    cases, 
    population, 
    cellsCoarse, 
    cellsFine, 
    bw,
    xv = 4, 
    lemObjects, 
    ncores = 1, 
    tol = 1e-6, 
    maxIter = 2000, 
    fact = 1,
    randomSeed = NULL, 
    verbose = FALSE,
    path = getwd()
){
  
  dir.create(path, showWarnings=FALSE, recursive=TRUE)
  
  # forcross-validation scores
  if(!is.null(randomSeed)) {
    set.seed(randomSeed)
  }
  
  if(missing(lemObjects)) {  
    if(verbose) {
      cat("computing smoothing matrix\n")
    }

    if(ncores > 1)
      spatial.tools::sfQuickInit(ncores, methods = TRUE, 
        .packages = c('Matrix','raster'))
    
    ##raster partition
    xvLemRaster = rasterPartition(
        polyCoarse = cases, 
        polyFine = population, 
        cellsCoarse = cellsCoarse, 
        cellsFine = cellsFine, 
        xv = xv,
        bw = bw,
        fact = fact,
        ncores = 0, 
        path = path,
        idFile = file.path(path,'idXv.grd'), 
        offsetFile = file.path(path, 'offsetXv.grd'), 
        verbose = verbose)
    
    xvSmoothMat =  smoothingMatrix(
        rasterObjects = xvLemRaster, 
        ncores = 0, 
        filename = file.path(path, 'smoothingMatrix.grd'),
        verbose = verbose)
    
    if(ncores > 1)
      spatial.tools::sfQuickStop()   
    
    xvMat = xvSmoothMat$xv
    
  } else {
    if(verbose) {
      cat("using supplied smoothing matrix\n")
    }
    
    xvSmoothMat = lemObjects
    xvMat = lemObjects$xv
  }
  
  if(is.vector(cases)) {
    cases = data.frame(cases=cases)
  }
  #names of interest for regions in the coarse shapefile
  countcol = grep('^(count|cases)[[:digit:]]+?$', 
      names(cases), value=TRUE, ignore.case=TRUE)
  if(!length(countcol)){
    countcol = grep(
        "^(id|name)", names(cases), 
        invert=TRUE, value=TRUE
    )[1]
  }
  if(class(cases) == 'SpatialPolygonsDataFrame') {
    polyCoarse = cases
    cases = cases@data
  } else {
    polyCoarse = NULL
  }
  
  if(verbose) {
    cat("running local-EM for validation sets\n")
  }
  # estimate risk (by partition, not continuous) for each bw/cv combinantion
#  estList = parallel::mcmapply(
  
  if(ncores > 1) spatial.tools::sfQuickInit(
      ncores, 
      methods = TRUE,
      .packages = c('Matrix','localEM', 'raster'))

  estList = foreach::foreach(
          bw = xvSmoothMat$bw,
          .packages=c('raster','localEM', 'Matrix')) %dopar% {
       try(riskEst(bw,
            x=cases[,countcol, drop=FALSE],
            lemObjects = xvSmoothMat,
            tol = tol, 
            maxIter =maxIter))
      }
  if(ncores > 1) spatial.tools::sfQuickStop()
  names(estList) = xvSmoothMat$bw
  
  if(any(unlist(lapply(estList, class)) == 'try-error') ) {
    warning("errors in local-em estimation")
    return(list(
            estList = estList,
            smoothingMatrix = xvSmoothMat,
            expected = polyCoarse,
            folds = xvMat
        ))
  }
  
  estListExp = try(lapply(estList, function(x) x$expected))
  
  if(class(estListExp) == "try-error") {
    return(list(
            xv = NULL,
            xvFull = NULL,
            smoothingMatrix = xvSmoothMat,
            expected = polyCoarse,
            folds = xvMat
        ))
  }
  
  estListRisk = lapply(estList, function(x) x$risk)
  
  estDf = as.matrix(do.call(cbind, estListExp))
  colnames(estDf)
  riskDf = as.matrix(do.call(cbind, estListRisk))
  colnames(estDf) = colnames(riskDf) = paste(
    rep(names(estList), unlist(lapply(estListExp, function(xx) dim(xx)[2]))),
          unlist(lapply(
      estListExp, colnames
      )), sep='_')
  rownames(riskDf) = colnames(xvSmoothMat$regionMat)
  
  if(verbose) {
    cat("computing CV scores\n")
  }
  # compute the CV scores
  
  # expected counts in left out regions
  xvEst = estDf[,grep("xv[[:digit:]]+", colnames(estDf))]
  Sxv = gsub("^bw[[:digit:]]+xv|_count[[:digit:]]+?$|_$", "", colnames(xvEst))
  Scount = gsub("bw[[:digit:]]+xv[[:digit:]]+_", "", colnames(xvEst))
  if(all(nchar(Scount)==0)) Scount = rep_len(countcol, length(Scount))
  Sbw = gsub('^bw|xv.*', "", colnames(xvEst))
  
  suppressMessages(xvEstMask <- xvMat[,Sxv] * xvEst)
  # observed counts in left out regions
  suppressMessages(xvObs <- xvMat[,Sxv] * as.matrix(cases[,Scount]))
  
  logProbCoarse = stats::dpois(as.matrix(xvObs), as.matrix(xvEstMask), log=TRUE)
  logProb = apply(logProbCoarse, 2, sum)
  
  logProbFull = data.frame(
      bw = as.numeric(Sbw),
      cases = Scount, 
      fold = Sxv, 
      minusLogProb = -logProb
  )
  
  xvRes = stats::aggregate(
      logProbFull[,'minusLogProb'],
      as.list(logProbFull[,c('bw','cases')]),
      sum
  )
  xvRes = stats::reshape(
      xvRes, direction = 'wide',
      idvar = 'bw',
      timevar = 'cases'
  )
  colnames(xvRes) = gsub("^x[.]", "", colnames(xvRes))    
  xvRes = xvRes[,c('bw',countcol)]
  minXvScore = apply(xvRes[,countcol, drop=FALSE],2,min)
  xvRes[,countcol] = xvRes[,countcol] - 
      matrix(minXvScore, nrow=nrow(xvRes), ncol=length(minXvScore), byrow=TRUE)
  if(verbose) {
    cat("putting estimated risk in raster\n")
  }
#  stuff <<- list(xvSmoothMat$rasterFine, riskDf) 
  newDf <- riskDf[
    as.character(raster::levels(xvSmoothMat$rasterFine)[[1]]$partition),]
  
  riskRaster = xvSmoothMat$rasterFine
  levels(riskRaster)[[1]] =  as.data.frame(cbind(
    ID = raster::levels(xvSmoothMat$rasterFine)[[1]]$ID,
    newDf))
    
  if(verbose) {
    cat("done\n")
  }
  
  result = list(
      xv = xvRes,
      xvFull = logProbFull,
      risk = riskRaster,
      smoothingMatrix = xvSmoothMat,
      expected = polyCoarse,
      folds = xvMat
  )
  return(result)
  
  
  
  
}



