#' @title Computes the likelihood cross-validation 
#'
#' @description The \code{lemXv} function computes the likelihood cross-validation scores for the observed data with the input bandwidths used for the smoothing matrix. The cross-valiation test and training datasets of the observed cases are generated by k-fold sampling without replacement. 
#' 
#' @param cases, 
#' @param population, 
#' @param cellsCoarse, 
#' @param cellsFine, 
#' @param bw,
#' @param xv Number of cross-validation datasets
#' @param lemObjects List of arrays for the smoothing matrix, and raster stacks for the partition and smoothed offsets
#' @param ncores Number of cores/threads for parallel processing
#' @param tol Tolerance for convergence
#' @param maxIter Maximum number of iterations for convergence
#' @param randomSeed Seed for random number generator
#' @param verbose Verbose output
#' @param path folder for storing rasters
#' 
#' 
#' @details After using the \code{lemXv} function, the last entry of the data frame object is the theoretical cross-validation score when bandwidth is set to infinity. 

#' @return The \code{lemXv} function returns a data frame of specified bandwidths and their cross-validation scores. 
#'  
#' @examples 
#' \dontrun{ 
#' data(kentuckyCounty)
#' data(kentuckyTract)
#' 
#' ncores = 1 + (.Platform$OS.type == 'unix')
#' 
#' lemRaster = rasterPartition(polyCoarse = kentuckyCounty, 
#'                            polyFine = kentuckyTract, 
#'                            cellsCoarse = 6, 
#'                            cellsFine = 100, 
#'                            bw = c(10, 12, 15, 17, 20, 25) * 1000, 
#'                            ncores = ncores, 
#'                            idFile = 'id.grd', 
#'                            offsetFile = 'offset.grd', 
#'                            verbose = TRUE)
#'
#'
#' lemSmoothMat = smoothingMatrix(rasterObjects = lemRaster, 
#'                                ncores = ncores, 
#'                                verbose = TRUE)
#'
#' lemCv = lemXv(x = kentuckyCounty, 
#'              lemObjects = lemSmoothMat, 
#'              Nxv = 5, 
#'              ncores = ncores, 
#'              verbose = TRUE)
#' bestBw = lemCv$bw[which.min(lemCv$cv)]
#'
#' plot(lemCv[is.finite(lemCv$bw),], 
#'     xlab = 'Bandwidth (m)', 
#'     ylab = 'CV', 
#'     log = 'x', col = 'red', type = 'o')
#' abline(v = bestBw, lty = 3)
#'}
#'
#' Computes the likelihood cross-validation by removing spatial regions and estimating risk for those regions
#' This requires computations of new smoothing matrix for each training set
#' Sampling Ratio: 75/25% of data for training and testing sets
#' @export
lemXv = function(
    cases, 
    population, 
    cellsCoarse, 
    cellsFine, 
    bw,
    xv = 4, 
    lemObjects, 
    ncores = 1, 
    tol = 1e-6, 
    maxIter = 2000, 
    fact = 1,
    randomSeed = NULL, 
    verbose = FALSE,
    path = getwd()
){
  
  dir.create(path, showWarnings=FALSE, recursive=TRUE)
  
  # forcross-validation scores
  if(!is.null(randomSeed)) {
    set.seed(randomSeed)
  }
  
  if(missing(lemObjects)) {  
    if(verbose) {
      cat("computing smoothing matrix\n")
    }

    if(ncores > 1)
      spatial.tools::sfQuickInit(ncores, methods = TRUE, 
        .packages = c('Matrix','raster'))
    
    ##raster partition
    xvLemRaster = rasterPartition(
        polyCoarse = cases, 
        polyFine = population, 
        cellsCoarse = cellsCoarse, 
        cellsFine = cellsFine, 
        xv = xv,
        bw = bw,
		      fact = fact,
        ncores = 0, 
        path = path,
        idFile = file.path(path,'idXv.grd'), 
        offsetFile = file.path(path, 'offsetXv.grd'), 
        verbose = verbose)
    
    xvSmoothMat =  smoothingMatrix(
        rasterObjects = xvLemRaster, 
        ncores = 0, 
        filename = file.path(path, 'smoothingMatrix.grd'),
        verbose = verbose)
    
    if(ncores > 1)
      spatial.tools::sfQuickStop()   
    
    xvMat = xvSmoothMat$xv
    
  } else {
    if(verbose) {
      cat("using supplied smoothing matrix\n")
    }
    
    xvSmoothMat = lemObjects
    xvMat = lemObjects$xv
  }
  
  if(is.vector(cases)) {
    cases = data.frame(cases=cases)
  }
  #names of interest for regions in the coarse shapefile
  countcol = grep('^(count|cases)[[:digit:]]+?$', 
      names(cases), value=TRUE, ignore.case=TRUE)
  if(!length(countcol)){
    countcol = grep(
        "^(id|name)", names(cases), 
        invert=TRUE, value=TRUE
    )[1]
  }
  if(class(cases) == 'SpatialPolygonsDataFrame') {
    polyCoarse = cases
    cases = cases@data
  } else {
    polyCoarse = NULL
  }
  
  if(verbose) {
    cat("running local-EM for validation sets\n")
  }
  # estimate risk (by partition, not continuous) for each bw/cv combinantion
#  estList = parallel::mcmapply(
  
  if(ncores > 1) spatial.tools::sfQuickInit(
      ncores, 
      methods = TRUE,
      .packages = c('Matrix','raster'))

  estList = foreach::foreach(
          bw = xvSmoothMat$bw,
          .export = 'riskEst', .packages=c('raster','Matrix')) %dopar% {
stuff=        try(riskEst(bw,
            x=cases[,countcol, drop=FALSE],
            lemObjects = xvSmoothMat,
            tol = tol, 
            maxIter =maxIter))
      }
  if(ncores > 1) spatial.tools::sfQuickStop()
  names(estList) = xvSmoothMat$bw
  
  if(any(unlist(lapply(estList, class)) == 'try-error') ) {
    return(list(
            estList = estList,
            smoothingMatrix = xvSmoothMat,
            expected = polyCoarse,
            folds = xvMat
        ))
  }
  
  estListExp = try(lapply(estList, function(x) x$expected))
  
  if(class(estListExp) == "try-error") {
    return(list(
            xv = NULL,
            xvFull = NULL,
            smoothingMatrix = xvSmoothMat,
            expected = polyCoarse,
            folds = xvMat
        ))
  }
  
  estListRisk = lapply(estList, function(x) x$risk)
  
  estDf = as.matrix(do.call(cbind, estListExp))
  riskDf = as.matrix(do.call(cbind, estListRisk))
  colnames(estDf) = colnames(riskDf) = paste(
    rep(names(estList), unlist(lapply(estListExp, function(xx) dim(xx)[2]))),
          unlist(lapply(
      estListExp, colnames
      )), sep='_')
  rownames(riskDf) = colnames(xvSmoothMat$regionMat)
  
  if(verbose) {
    cat("computing CV scores\n")
  }
  # compute the CV scores
  
  # expected counts in left out regions
  xvEst = estDf[,grep("xv[[:digit:]]+", colnames(estDf))]
  Sxv = gsub("^bw[[:digit:]]+xv|_count[[:digit:]]+?$|_$", "", colnames(xvEst))
  Scount = gsub("bw[[:digit:]]+xv[[:digit:]]+_", "", colnames(xvEst))
  if(all(nchar(Scount)==0)) Scount = rep(countcol, length(Scount))
  Sbw = gsub('^bw|xv.*', "", colnames(xvEst))
  
  suppressMessages(xvEstMask <- xvMat[,Sxv] * xvEst)
  # observed counts in left out regions
  suppressMessages(xvObs <- xvMat[,Sxv] * as.matrix(cases[,Scount]))
  
  logProbCoarse = stats::dpois(as.matrix(xvObs), as.matrix(xvEstMask), log=TRUE)
  logProb = apply(logProbCoarse, 2, sum)
  
  logProbFull = data.frame(
      bw = as.numeric(Sbw),
      cases = Scount, 
      fold = Sxv, 
      minusLogProb = -logProb
  )
  
  xvRes = aggregate(
      logProbFull[,'minusLogProb'],
      as.list(logProbFull[,c('bw','cases')]),
      sum
  )
  xvRes = reshape(
      xvRes, direction = 'wide',
      idvar = 'bw',
      timevar = 'cases'
  )
  colnames(xvRes) = gsub("^x[.]", "", colnames(xvRes))    
  xvRes = xvRes[,c('bw',countcol)]
  minXvScore = apply(xvRes[,countcol, drop=FALSE],2,min)
  xvRes[,countcol] = xvRes[,countcol] - 
      matrix(minXvScore, nrow=nrow(xvRes), ncol=length(minXvScore), byrow=TRUE)
  if(verbose) {
    cat("putting estimated risk in raster\n")
  }
#  stuff <<- list(xvSmoothMat$rasterFine, riskDf) 
  newDf <- riskDf[
    as.character(raster::levels(xvSmoothMat$rasterFine)[[1]]$partition),]
  
  riskRaster = xvSmoothMat$rasterFine
  levels(riskRaster)[[1]] =  as.data.frame(cbind(
    ID = raster::levels(xvSmoothMat$rasterFine)[[1]]$ID,
    newDf))
    
  if(verbose) {
    cat("done\n")
  }
  
  result = list(
      xv = xvRes,
      xvFull = logProbFull,
      risk = riskRaster,
      smoothingMatrix = xvSmoothMat,
      expected = polyCoarse,
      folds = xvMat
  )
  return(result)
  
  
  
  
}





## Computes the expected counts for the test set based on aggregated risk estimation and smoothing matrix of training set
xvLemEstOneBw = function(
    Dbw,
    trainId,
    trainCounts,
    regionOffset, # = crossprod(regionMat, offsetMat)
    smoothingMat,
    regionXvOffset, # = crossprod(regionMat, xvOffsetMat) 
    startingValue,
    tol,
    maxIter) {
  
  xvLambda = xvLemEst(
      trainId=trainId,
      trainCounts=trainCounts,
      regionOffset =regionOffset,
      smoothingMat = Matrix(drop(smoothingMat[,,Dbw])),
      startingValue = startingValue,
      tol=tol,
      maxIter = maxIter)
  
  as.matrix(regionXvOffset %*% xvLambda)
  
}

## Computes the aggregated risk estimation of the training set
xvLemEst = function(
    trainId,
    trainCounts,
    regionOffset,
    smoothingMat,
    offsetMat,
    startingValue = matrix(
        apply(regionOffset, 2, sum),
        ncol(regionOffset), length(trainId)
    ),
    tol,
    maxIter) {
  
  if(missing(trainId))
    trainId = 1:ncol(trainCounts)
  obsCounts = as.matrix(trainCounts[,trainId, drop=FALSE])
  
  oldRes = startingValue
  
  Diter = 1
  absdiff = 1
  
  while((absdiff > tol) && (Diter < maxIter)) {
    
    newRes = oneLemIter(
        Lambda = oldRes,
        smoothingMat = smoothingMat,
        regionOffset = regionOffset,
        counts = obsCounts
    )
    
    absdiff = mean(abs(oldRes - newRes))
    
    oldRes = newRes
    Diter = Diter + 1
  }
  
  result = as.matrix(newRes)
  
  return(result)
}
