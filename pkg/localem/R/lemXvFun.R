#' @title Computes the likelihood cross-validation 
#'
#' @description The \code{lemXv} function computes the likelihood cross-validation scores for the observed data with the input bandwidths used for the smoothing matrix. The cross-valiation test and training datasets of the observed cases are generated by k-fold sampling without replacement. 
#' 
#' @param x Spatial polygons of case data
#' @param lemObjects List of arrays for the smoothing matrix, and raster stacks for the partition and smoothed offsets
#' @param Nxv Number of cross-validation datasets
#' @param ncores Number of cores/threads for parallel processing
#' @param tol Tolerance for convergence
#' @param maxIter Maximum number of iterations for convergence
#' @param randomSeed Seed for random number generator
#' @param verbose Verbose output
#' 
#' @importFrom stats dpois rmultinom
#' @import sp 
#' @import Matrix 
#' @import raster

#' 
#' @details After using the \code{lemXv} function, the last entry of the data frame object is the theoretical cross-validation score when bandwidth is set to infinity. 

#' @return The \code{lemXv} function returns a data frame of specified bandwidths and their cross-validation scores. 
#'  
#' @examples 
#' \dontrun{ 
#' data(kentuckyCounty)
#' data(kentuckyTract)
#' 
#' ncores = 1 + (.Platform$OS.type == 'unix')
#' 
#' lemRaster = rasterPartition(polyCoarse = kentuckyCounty, 
#'                            polyFine = kentuckyTract, 
#'                            cellsCoarse = 6, 
#'                            cellsFine = 100, 
#'                            bw = c(10, 12, 15, 17, 20, 25) * 1000, 
#'                            ncores = ncores, 
#'                            idFile = 'id.grd', 
#'                            offsetFile = 'offset.grd', 
#'                            verbose = TRUE)
#'
#'
#' lemSmoothMat = smoothingMatrix(rasterObjects = lemRaster, 
#'                                ncores = ncores, 
#'                                verbose = TRUE)
#'
#' lemCv = lemXv(x = kentuckyCounty, 
#'              lemObjects = lemSmoothMat, 
#'              Nxv = 5, 
#'              ncores = ncores, 
#'              verbose = TRUE)
#' bestBw = lemCv$bw[which.min(lemCv$cv)]
#'
#' plot(lemCv[is.finite(lemCv$bw),], 
#'     xlab = 'Bandwidth (m)', 
#'     ylab = 'CV', 
#'     log = 'x', col = 'red', type = 'o')
#' abline(v = bestBw, lty = 3)
#'}
#'
#' @export
#' Computes the likelihood cross-validation by removing spatial regions and estimating risk for those regions
#' This requires computations of new smoothing matrix for each training set
#' Sampling Ratio: 75/25% of data for training and testing sets
lemXv = function(
    polyCoarse, 
    polyFine, 
    cellsCoarse, 
    cellsFine, 
    bw,
    Nxv = 4, 
    ncores = 1, 
    tol = 1e-6, 
    maxIter = 2000, 
    randomSeed = NULL, 
    verbose = FALSE,
    path = getwd()
){
  
  # forcross-validation scores
  if(!is.null(randomSeed)) {
    set.seed(randomSeed)
  }
  
  
  ##raster partition
  xvLemRaster = rasterPartition(
      polyCoarse = polyCoarse, 
      polyFine = polyFine, 
      cellsCoarse = cellsCoarse, 
      cellsFine = cellsFine, 
      xv = Nxv,
      bw = bw, 
      ncores = ncores, 
      idFile = file.path(path,'idXv.grd'), 
      offsetFile = file.path(path, 'offsetXv.grd'), 
      verbose = verbose)
  
  xvSmoothMat =  smoothingMatrix(
      rasterObjects = xvLemRaster, 
      ncores = ncores, 
      verbose = verbose)
  
  
  #names of interest for regions in the coarse shapefile
  countcol = grep('^(count|cases)$', names(polyCoarse), value=TRUE, ignore.case=TRUE)
  if(length(countcol)){
    countcol = countcol[1]
  } else {
    countcol = grep(
        "^(id|name)", names(polyCoarse), 
        invert=TRUE, value=TRUE
    )[1]
  }
  
  idColX = grep("^id", names(polyCoarse), value=TRUE)
  if(length(idColX)) {
    idColX = idColX[1]
  } else {
    idColX = names(polyCoarse)[1]
  }
  
  # estimate risk (by partition, not continuous) for each bw/cv combinantion
  estList = mcmapply(
      riskEst,
      bw = dimnames(xvSmoothMat$smoothingArray)[[3]],
      MoreArgs = list(
          x=polyCoarse@data[,countcol],
          lemObjects = xvSmoothMat,
          tol = tol, 
          maxIter =maxIter,
          ncores = NULL,
          final = FALSE),
      SIMPLIFY=FALSE
  )
  estDf = do.call(cbind, estList)
  colnames(estDf) = names(estList)
  
  # now compute the CV scores

  xvPartitions = xvSmoothMat$xv[levels(xvSmoothMat$rasterFine)[[1]][,'idCoarse'], ]

  # old code below.
  
  
  startingValue = simplify2array(lapply(xvSmoothMat$offsetMat, 
          function(x) apply(x, 2, sum)))
  rownames(startingValue) = colnames(xvSmoothMat$offsetMat[[1]])
  colnames(startingValue) = gsub('[oO]ffset', '', colnames(startingValue))
  startingValue = startingValue[, 
      match(gsub('^bw[[:digit:]]+', '', dimnames(xvSmoothMat$smoothingArray)[[3]]),
          colnames(startingValue))]
  colnames(startingValue) = dimnames(xvSmoothMat$smoothingArray)[[3]]
  
  
  
  xvList = parallel::mcmapply(
      xvLemEstOneBw, 
      trainId = 1, 
      trainCounts = trainCounts, 
      regionOffset = trainRegionOffset, 
      smoothingMat = trainSmoothingMat, 
      regionXvOffset = xvRegionOffset, 
      startingValue = startingValue, 
      tol = tol, 
      maxIter = maxIter, 
      mc.cores = ncores
  )
  
  
#linkage for regions in the coarse and fine shapefiles (based on raster)
  polyFineCentre = SpatialPoints(polyFine, proj4string = polyFine@proj4string)
  polyFineCell = cellFromXY(xvRasterFine, polyFineCentre@coords)
  
  trainLemSmoothMat = trainLemRaster = vector('list', Nxv)
  
  
#counts
  x = polyCoarse
  if(class(x) == 'SpatialPolygonsDataFrame') {
    x = data.frame(x)
  }
  
  for(inXv in 1:Nxv) {
    if(verbose) cat("cv ", inXv, '\n')
    # reset training set
    ##offset and smoothing matrix
    trainPolyFine = polyFine
    
    #test set
    ##random sample (25% used as testing data)
    xvId = which(xvIdMat[,inXv])
    
    trainRasterIdCoarse = which(values(xvRasterFine[['idCoarse']]) %in% xvId)
    trainPolyFine$expected[polyFineCell %in% trainRasterIdCoarse] = 0
    
    if(is.null(trainLemSmoothMat[[inXv]])) {
      trainLemRaster[[inXv]] = rasterPartition(
          polyCoarse = polyCoarse, 
          polyFine = trainPolyFine, 
          cellsCoarse = cellsCoarse, 
          cellsFine = cellsFine, 
          bw = bw,
          ncores = ncores, 
          idFile = file.path(path, paste('idTrain',inXv, '.grd', sep='')),
          offsetFile = file.path(path, paste('offsetTrain',inXv, '.grd', sep='')),
          verbose = verbose)
    }
  } 
  
  trainLemRaster[[inXv]]$rasterFine = xvRasterFine
  
  trainLemSmoothMat[[inXv]] = smoothingMatrix(
      rasterObjects = trainLemRaster[[inXv]], 
      ncores = ncores, 
      verbose = verbose)
  
  trainOffsetMat = trainLemSmoothMat[[inXv]]$offsetMat
  trainSmoothingMat = trainLemSmoothMat[[inXv]]$smoothingArray
  Spartitions = dimnames(trainSmoothingMat)[[1]]
  
#test
  ##offsets
  xvRasterOffsets = xvLemRaster$offset[['offset']]
  values(xvRasterOffsets)[!(values(xvRasterFine[['idCoarse']]) %in% xvId)] = 0
  
  xvMeanOffsets = tapply(values(xvRasterOffsets), 
      list(Scells), 
      mean, na.rm=TRUE)
  
  xvMeanOffsets = xvMeanOffsets[match(Spartitions, names(xvMeanOffsets))]
  xvOffsetMat = Diagonal(length(Spartitions), xvMeanOffsets)
  
  ##fine raster did not include all regions in the coarse shapefile
  regionMat = trainLemSmoothMat[[inXv]]$regionMat
  
  if(length(idCoarse) != dim(regionMat)[[2]]) {
    
    polyNeigh = spdep::poly2nb(trainLemSmoothMat[[inXv]]$polyCoarse, 
        row.names = idCoarse)
    
    idMatch = idCoarse[as.numeric(dimnames(regionMat)[[2]])]
    idNotMatch = idCoarse[!(idCoarse %in% idMatch)]
    
    countCoarse = x[match(idMatch, x[[idColX]]),countcol]
    
    for(inD in idNotMatch) {
      
      polyNotMatch = trainLemSmoothMat[[inXv]]$polyCoarse[idCoarse == inD,]
      idNeighNotMatch = idCoarse[values(intersect(
                  trainLemSmoothMat[[inXv]]$rasterFine[["idCoarse"]],
                  polyNotMatch))]
      idNeighNotMatch = idNeighNotMatch[!is.na(idNeighNotMatch)]
      
      #if no match found in fine raster, use neighbouring coarse shapefile regions 
      if(length(idNeighNotMatch) == 0) {
        idNeighNotMatch = idCoarse[polyNeigh[[which(idCoarse == inD)]]]
        idNeighNotMatch = idMatch[idMatch %in% idNeighNotMatch]
      }
      
      #re-assign counts
      if(length(idNeighNotMatch) == 1) {
        
        countCoarse[idMatch == idNeighNotMatch] = 
            countCoarse[idMatch == idNeighNotMatch] + x[x[[idColX]] == inD,countcol]
        
      } else if(length(idNeighNotMatch) > 1) {
        
        #if conflict, assign counts to coarse shapefile region whose centroid is closest to the one of interest
        polyNeighNotMatch = trainLemSmoothMat[[inXv]]$polyCoarse[
            idCoarse %in% idNeighNotMatch,]
        coordsNeighNotMatch = coordinates(rgeos::gCentroid(polyNeighNotMatch, byid = TRUE))
        
        coordsNotMatch = matrix(
            rep(coordinates(rgeos::gCentroid(polyNotMatch, byid = TRUE)), each = length(polyNeighNotMatch)), 
            nrow = length(polyNeighNotMatch), 
            ncol = 2, 
            dimnames = list(1:length(polyNeighNotMatch), c("x","y"))
        )
        
        distNeighNotMatch = apply((coordsNeighNotMatch - coordsNotMatch)^2, 1, sum)
        
        countCoarse[
            idMatch == idNeighNotMatch[which.min(distNeighNotMatch)]
        ] = countCoarse[
                idMatch == idNeighNotMatch[which.min(distNeighNotMatch)]
            ] + x[x[[idColX]] == inD,countcol]
        
      }
    }
    
    idCoarse = idMatch
    
  } else {
    countCoarse = x[match(idCoarse, x[[idColX]]),countcol]
  }
  
  
#risk estimation
  ##finite bandwidths
  if(verbose) {
    cat(date(), "\n")
    cat("obtaining CV for finite bandwidths\n")
  }
  
  ##training counts of interest
  trainCounts = as.matrix(countCoarse)
  trainCounts[idCoarse %in% xvId,] = 0
  
  ##starting values
  trainRegionOffset = crossprod(regionMat, trainOffsetMat)
  xvRegionOffset = crossprod(regionMat, xvOffsetMat) 
  
  startingValue = matrix(apply(trainRegionOffset, 2, sum), 
      ncol(trainRegionOffset), 1)
  
  xvList = parallel::mclapply(dimnames(trainSmoothingMat)[[3]], 
      xvLemEstOneBw, 
      trainId = 1, 
      trainCounts = trainCounts, 
      regionOffset = trainRegionOffset, 
      smoothingMat = trainSmoothingMat, 
      regionXvOffset = xvRegionOffset, 
      startingValue = startingValue, 
      tol = tol, 
      maxIter = maxIter, 
      mc.cores = ncores
  )
  
  names(xvList) = dimnames(trainSmoothingMat)[[3]]
  xvMeans = simplify2array(xvList)[,1,]
  
#cross-validation scores
  ##finite bandwidths
  
  ##test counts of interest
  xvCounts = matrix(countCoarse, length(countCoarse), dim(trainSmoothingMat)[3])
  xvCounts[!(idCoarse %in% xvId),] = 0
  
  xvRes = dpois(xvCounts, xvMeans, log = TRUE)
  dimnames(xvRes) = list(1:length(countCoarse), dimnames(trainSmoothingMat)[[3]])
  xvCv[dimnames(xvRes)[[2]],inXv] = -apply(xvRes, 2, sum)
  
  
#risk estimation
  ##infinite bandwidth		
  if(verbose) {
    cat(date(), "\n")
    cat("obtaining CV for infinite bandwidth\n")
  }
  
  Scw2 = prod(res(xvRasterFine))
  SNcells = tapply(Scells, list(Scells), length)
  SNcells = SNcells[match(dimnames(trainSmoothingMat)[[1]], names(SNcells))]
  Sarea = SNcells * Scw2 
  
  Sexpected = trainOffsetMat@x * Sarea
  
  xvLambdaInf = as.matrix(Sarea * sum(trainCounts) / sum(Sexpected))
  
#cross-validation scores
  ##infinite bandwidths
  xvMeansInf = xvRegionOffset %*% xvLambdaInf
  
  ##test counts of interest
  xvCountsInf = matrix(countCoarse, length(countCoarse), 1)
  xvCountsInf[!(idCoarse %in% xvId),] = 0
  
  xvResInf = dpois(as.vector(xvCountsInf), as.vector(xvMeansInf), log = TRUE)
  xvCv["bwInf",inXv] = -sum(xvResInf)
  
  theXv = apply(xvCv, 1, sum)
  
  result = data.frame(
      bw = as.numeric(gsub("^bw", "", names(theXv))), 
      cv = theXv
  )
  
  if(verbose) { 
    cat(date(), "\n")
    cat("done\n")
  }
  
  return(result)
  
}




#' Additional functions from the 'localEM' package that are required but were not globally loaded
## Computes the risk estimation aggregated to the partitions for one iteration
oneLemIter = function(
    Lambda, smoothingMat, regionMat, offsetMat, counts,
    regionOffset = crossprod(regionMat, offsetMat)
){
  
  denom = regionOffset %*% Lambda
  
  em = crossprod(regionOffset, counts/denom) * Lambda
  em[as.vector(!is.finite(em))] = 0
  
  result = crossprod(smoothingMat, em)
  attributes(result)$em = em
  
  return(result)
}

## Computes the expected counts for the test set based on aggregated risk estimation and smoothing matrix of training set
xvLemEstOneBw = function(
    Dbw,
    trainId,
    trainCounts,
    regionOffset, # = crossprod(regionMat, offsetMat)
    smoothingMat,
    regionXvOffset, # = crossprod(regionMat, xvOffsetMat) 
    startingValue,
    tol,
    maxIter) {
  
  xvLambda = xvLemEst(
      trainId=trainId,
      trainCounts=trainCounts,
      regionOffset =regionOffset,
      smoothingMat = Matrix(drop(smoothingMat[,,Dbw])),
      startingValue = startingValue,
      tol=tol,
      maxIter = maxIter)
  
  as.matrix(regionXvOffset %*% xvLambda)
  
}

## Computes the aggregated risk estimation of the training set
xvLemEst = function(
    trainId,
    trainCounts,
    regionOffset,
    smoothingMat,
    offsetMat,
    startingValue = matrix(
        apply(regionOffset, 2, sum),
        ncol(regionOffset), length(trainId)
    ),
    tol,
    maxIter) {
  
  if(missing(trainId))
    trainId = 1:ncol(trainCounts)
  obsCounts = as.matrix(trainCounts[,trainId, drop=FALSE])
  
  oldRes = startingValue
  
  Diter = 1
  absdiff = 1
  
  while((absdiff > tol) && (Diter < maxIter)) {
    
    newRes = oneLemIter(
        Lambda = oldRes,
        smoothingMat = smoothingMat,
        regionOffset = regionOffset,
        counts = obsCounts
    )
    
    absdiff = mean(abs(oldRes - newRes))
    
    oldRes = newRes
    Diter = Diter + 1
  }
  
  result = as.matrix(newRes)
  
  return(result)
}
